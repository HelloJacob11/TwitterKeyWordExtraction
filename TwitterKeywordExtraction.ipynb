{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HelloJacob11/TwitterKeyWordExtraction/blob/main/TwitterKeywordExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter KeyWord Extraction\n",
        "## This program traverses through 1,000 tweets per minute and finds any tweet that has the word \"Democrat\" or \"Republican\". Once the program finds tweet with one of those words, it traverses through other tweets that the user of that specific tweet posted to extract any keyword associated with either \"Democrat\" or \"Republican\".\n",
        "- *Project Work Duration: May 2022 - September 2022*"
      ],
      "metadata": {
        "id": "Kj7SBaCGCOLv"
      },
      "id": "Kj7SBaCGCOLv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782b177f",
      "metadata": {
        "scrolled": false,
        "id": "782b177f"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "API_key=\"Personal API Key\"\n",
        "API_key_secret=\"API Secret Key\"\n",
        "Bearer_Token\"Personal Bearer Token\"\n",
        "Access_Token=\"Personal Access Token\"\n",
        "Access_Token_Secret=\"Personal Secret Access Token\"\n",
        "def get_tweets(keyword):\n",
        "    auth=tweepy.OAuthHandler(API_key,API_key_secret)\n",
        "\n",
        "    auth.set_access_token(Access_Token,Access_Token_Secret)\n",
        "\n",
        "    api=tweepy.API(auth)\n",
        "    #print(api)\n",
        "    print(keyword)\n",
        "    ans =[]\n",
        "    tweets = api.search_tweets(q = keyword,count = 1000)\n",
        "    for tweet in tweets:\n",
        "        ans.append(tweet.text)\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "475a4a8c",
      "metadata": {
        "id": "475a4a8c"
      },
      "source": [
        "# This function removes all the unnecessary words in a tweet and converts the words in lowercase.\n",
        "- parameter: text from a tweet\n",
        "- return: returns a list that contains all the necessary words (words that could be used to extract keywords) in a tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d719f6ea",
      "metadata": {
        "scrolled": false,
        "id": "d719f6ea"
      },
      "outputs": [],
      "source": [
        "def remove_stopword(text):\n",
        "   # print(text)\n",
        "    text=text.replace(\"\\\"\",\"\")\n",
        "    text=text.replace(\"\\n\",\" \")\n",
        "    text=text.replace(\".\",\" \")\n",
        "    text=text.replace(\",\",\" \")\n",
        "    text=text.replace(\"!\",\" \")\n",
        "    text=text.replace(\"?\",\" \")\n",
        "    split_text=text.split(\" \")\n",
        "    a=open(\"stopword.txt\",'r')\n",
        "    STOPWORD=a.read().replace(\"\\\"\",\"\").split(\", \")\n",
        "    All=[]\n",
        "    for Any in split_text:\n",
        "        value =  Any.lower()\n",
        "        che = True\n",
        "        if len(Any)<=1:\n",
        "            che=False\n",
        "        if value not in STOPWORD and che==True:\n",
        "            for i in value:\n",
        "                if (i.isalpha()==False) and (i.isdigit()==False):\n",
        "                    che = False\n",
        "                    break\n",
        "            if che == True:\n",
        "                All.append(value)\n",
        "    return All"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa823835",
      "metadata": {
        "id": "fa823835"
      },
      "source": [
        "# This function finds how often a specific word appears in the word list.\n",
        "- parameter: a word and a word list\n",
        "- return: return a list that contains how often a specific word appears in the word list(parameter)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc321467",
      "metadata": {
        "id": "cc321467"
      },
      "outputs": [],
      "source": [
        "def frequency(words,wordlist):\n",
        "    for word in words:\n",
        "        if word not in wordlist:\n",
        "            wordlist[word]=0\n",
        "        wordlist[word]+=1\n",
        "    return wordlist\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a137ccf7",
      "metadata": {
        "id": "a137ccf7"
      },
      "source": [
        "# This function finds the stem of the words in the given word list(parameter). For instance, the stem of fancy is fan.\n",
        "- parameter: a word list\n",
        "- return: a list that contains all the \"stemmed\" words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1690671e",
      "metadata": {
        "id": "1690671e"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def stemming(wordList):\n",
        "    A=stem.PorterStemmer()\n",
        "    List=[]\n",
        "    for word in wordList:\n",
        "        List.append(A.stem(word))\n",
        "    return List"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc741345",
      "metadata": {
        "id": "bc741345"
      },
      "source": [
        "# This function converts a pos into readable pos.\n",
        "- parameter: part of speech of a specific word\n",
        "- return: readable pos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff949367",
      "metadata": {
        "id": "ff949367"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "def pos_translate(pos):\n",
        "    if pos=='J':\n",
        "        return 'a'\n",
        "    if pos==\"R\":\n",
        "        return 'r'\n",
        "    if pos=='V':\n",
        "        return 'v'\n",
        "    if pos=='N':\n",
        "        return 'n'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This function calls pos_translate to convert a pos of a specific word into a readable pos. Then, it \"lemmitizes\" the word and the readable pos in the word list.  \n",
        "- parameter: a word list\n",
        "- return: returns a list that contains all the words that are \"lemmitized\""
      ],
      "metadata": {
        "id": "cPPYZ26iBTC6"
      },
      "id": "cPPYZ26iBTC6"
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizer_ans(wordList):\n",
        "    List=[]\n",
        "    pos_words=pos_tag(wordList)\n",
        "   # print(pos_words)\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    for word in pos_words:\n",
        "        POS=pos_translate(word[1][0])\n",
        "        if (POS != None):\n",
        "            List.append(lemmatizer.lemmatize(word[0],POS))\n",
        "    return List"
      ],
      "metadata": {
        "id": "DQ4orIXaBXFL"
      },
      "id": "DQ4orIXaBXFL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902fe8f0",
      "metadata": {
        "id": "902fe8f0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def randomWord():\n",
        "    randomList = [[] for i in range(10)]  #[[],[],[]]\n",
        "    finalList=[]\n",
        "    a=open(\"randomWords.txt\",'r')\n",
        "    randomWords=a.read().split(\"\\n\")\n",
        "    idx = -1\n",
        "    for a in range(0,len(randomWords),1):\n",
        "        if len(randomWords[a]) >0 and randomWords[a][0]==\"(\":\n",
        "            idx+=1\n",
        "        else:\n",
        "            randomList[idx].append(randomWords[a])\n",
        "    #print(randomList)\n",
        "    for b in range(0,len(randomList),1):\n",
        "        value=random.choice(randomList[b])\n",
        "        finalList.append(value)\n",
        "    return finalList\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This function processes a list of word from tweets that contain either \"republican\" or \"democrats\" to get a list that contains words that can possibly be keywords.\n",
        "- parameters: a list of word from tweets that contain either \"republican\" or \"democrats\", a list that is going to be processed in the function to prevent same word from getting appended, a list that is used for traversing, a list containing words after traversal, a final list of possible candidates for keywords\n",
        "- return1: return a list of words that are possible candidates of keywords."
      ],
      "metadata": {
        "id": "FfCESwl3B47B"
      },
      "id": "FfCESwl3B47B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9d17c6",
      "metadata": {
        "id": "bc9d17c6"
      },
      "outputs": [],
      "source": [
        "def extraction1(getlistName,total_tweets,tweet_list,ans,out):\n",
        "    for a in getlistName:\n",
        "        if a not in total_tweets:\n",
        "            total_tweets.append(a)\n",
        "            tweet_list.append(a)\n",
        "    for tweet in tweet_list:\n",
        "        word_list = remove_stopword(tweet)\n",
        "        word_list = lemmatizer_ans(word_list)\n",
        "        ans = frequency(word_list,ans)\n",
        "\n",
        "    for word in ans:\n",
        "        if ans[word]>1:\n",
        "            out[word] = ans[word]\n",
        "\n",
        "    dict_word=list(out.items())\n",
        "    dict_word.sort(key=lambda x:x[1], reverse=True)\n",
        "    out=dict(dict_word)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "926f0b7e",
      "metadata": {
        "id": "926f0b7e"
      },
      "outputs": [],
      "source": [
        "#parameters: a list that went through extraction1 method, a list that contains random words from various fields of jobs.\n",
        "#function: this function traverses words in extractedList and removes words that match with random words from randomwordList.\n",
        "#return: returns a list that contains all the words that are considered keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# this function traverses words in extractedList and removes words that match with random words from randomwordList.\n",
        "- parameters: a list that went through extraction1 method, a list that contains random words from various fields of jobs.\n",
        "- return: returns a list that contains all the words that are considered keywords"
      ],
      "metadata": {
        "id": "WGkIAwjUB8fh"
      },
      "id": "WGkIAwjUB8fh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "647ff9d5",
      "metadata": {
        "id": "647ff9d5"
      },
      "outputs": [],
      "source": [
        "def extraction2(extractedList,randomwordList):\n",
        "    mustremovedwordList=[]\n",
        "    for i in range(len(extractedList),-1,-1):\n",
        "        for j in range(len(randomwordList),-1,-1):\n",
        "            if extractedList[i]==randomwordList[j]:\n",
        "                extractedList.remove(extractedList[i])\n",
        "    return extractedList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5394a8c0",
      "metadata": {
        "scrolled": true,
        "id": "5394a8c0",
        "outputId": "250b68d6-f4f5-4d3b-ca78-d5cf64a46b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "republican\n",
            "democrat\n",
            "Beach \n",
            "Truncated\n",
            "Collagraphs\n",
            "Robust\n",
            "relief\n",
            "Foundation\n",
            "flask\n",
            "crop\n",
            "Acquittal\n",
            "Quota\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "74",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m'''///republicans\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03mfor a in get_listR:\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    if a not in total_tweetsR:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03mprint()\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     50\u001b[0m R\u001b[38;5;241m=\u001b[39mextraction1(get_listR,total_tweetsR,tweet_listR,ansR,outR)\n\u001b[1;32m---> 51\u001b[0m R2\u001b[38;5;241m=\u001b[39m\u001b[43mextraction2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43mget_listRA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(R2)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m'''///democrats \u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03mfor a in get_listD:\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    if a not in total_tweetsD:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03mprint()\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
            "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mextraction2\u001b[1;34m(extractedList, randomwordList)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(extractedList),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(randomwordList),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mextractedList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m==\u001b[39mrandomwordList[j]:\n\u001b[0;32m      6\u001b[0m             extractedList\u001b[38;5;241m.\u001b[39mremove(extractedList[i])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extractedList\n",
            "\u001b[1;31mKeyError\u001b[0m: 74"
          ]
        }
      ],
      "source": [
        "import time\n",
        "total_tweetsR = []\n",
        "total_tweetsD=[]\n",
        "total_tweetsRA=[]\n",
        "ansR = {}\n",
        "ansD={}\n",
        "ansRA={}\n",
        "tweet_listR=[]\n",
        "tweet_listD=[]\n",
        "tweet_listRA=[]\n",
        "get_listR=get_tweets(\"republican\")\n",
        "get_listD=get_tweets(\"democrat\")\n",
        "Random=randomWord()\n",
        "get_listRA = []\n",
        "for f in range(0,10,1):\n",
        "    get_listRA += get_tweets(Random[f])\n",
        "\n",
        "outR={}\n",
        "outD={}\n",
        "outRA={}\n",
        "finalR={}\n",
        "finalD={}\n",
        "finalRA={}\n",
        "\n",
        "\n",
        "'''///republicans\n",
        "for a in get_listR:\n",
        "    if a not in total_tweetsR:\n",
        "        total_tweetsR.append(a)\n",
        "        tweet_listR.append(a)\n",
        "\n",
        "for tweet in tweet_listR:\n",
        "    word_listR = remove_stopword(tweet)\n",
        "    word_listR = lemmatizer_ans(word_listR)\n",
        "    ans = frequency(word_listR,ansR)\n",
        "\n",
        "for word in ansR:\n",
        "    if ansR[word]>1:\n",
        "        outR[word]=ansR[word]\n",
        "\n",
        "dictR = list(outR.items())\n",
        "dictR.sort(key = lambda x: x[1],reverse = True)\n",
        "outR = dict(dictR)\n",
        "\n",
        "\n",
        "print(\"Republican\"+\" \")\n",
        "print(outR)\n",
        "print()\n",
        "'''\n",
        "R=extraction1(get_listR,total_tweetsR,tweet_listR,ansR,outR)\n",
        "R2=extraction2(R,get_listRA)\n",
        "print(R2)\n",
        "'''///democrats\n",
        "for a in get_listD:\n",
        "    if a not in total_tweetsD:\n",
        "        total_tweetsD.append(a)\n",
        "        tweet_listD.append(a)\n",
        "for tweet in tweet_listD:\n",
        "   # print(tweet , type(tweet))\n",
        "    word_listD = remove_stopword(tweet)\n",
        "    word_listD = lemmatizer_ans(word_listD)\n",
        "    ansD = frequency(word_listD,ansD)\n",
        "\n",
        "for word in ansD:\n",
        "    if ansD[word]>1:\n",
        "        outD[word] = ansD[word]\n",
        "\n",
        "dictD=list(outD.items())\n",
        "dictD.sort(key=lambda x:x[1], reverse=True)\n",
        "outD=dict(dictD)\n",
        "\n",
        "print(\"Democrat\"+\" \")\n",
        "print(outD)\n",
        "print()\n",
        "'''\n",
        "'''///random\n",
        "for a in randomWord():\n",
        "    if a not in total_tweetsRA:\n",
        "        total_tweetsRA.append(a)\n",
        "        tweet_listRA.append(a)\n",
        "\n",
        "for tweet in tweet_listRA:\n",
        "    word_listRA=remove_stopword(tweet)\n",
        "    word_listRA=lemmatizer_ans(word_listRA)\n",
        "    ansRA=frequency(word_listRA,ansRA)\n",
        "\n",
        "for word in ansRA:\n",
        "    if ansRA[word]>1:\n",
        "        outRA[word] = ansRA[word]\n",
        "\n",
        "dictRA=list(outRA.items())\n",
        "dictRA.sort(key=lambda x:x[1], reverse=True)\n",
        "outRa=dict(dictRA)\n",
        "\n",
        "print(\"Random\"+\" \")\n",
        "print(outRA)\n",
        "print()\n",
        "'''\n",
        "D=extraction1(get_listD,total_tweetsD,tweet_listD,ansD,outD)\n",
        "D2=extraction2(D,get_listRA)\n",
        "print(D2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}